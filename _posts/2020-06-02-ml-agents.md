---
layout: post
title:  "Training my ML Agent with Unity"
date:   2020-06-18 23:56:10 -0800
categories: dts game
---

# Training my ML Agent with Unity

TODO:
- add a gif of how "zones" work

With *Rusty's Dump Truck Service* headed to the [App Store](https://apps.apple.com/us/app/id1513836293), I wanted to take Unity's ML agents for a spin and train them to pilot Rusty's truck to victory.

The goal is simple - survive as long as possible by avoiding running into an enemy or getting shot down.
The player can move the truck up and down to avoid enemies by dragging a finger. Whenever a finger is on the screen, the truck's gun fires, which destroys the alien baddies.

![Video of gameplay](
https://www.dropbox.com/s/br2qsnc6bo0d5hl/rustybot_heuristic.m4v?raw=1)

This is simple enough that adapting a RustyBot to pilot the truck should be dead simple!

Unity has an intro to the different kinds of ML agents/what they are here: https://unity3d.com/how-to/unity-machine-learning-agents

Dump Truck Service is a single-player game, so I'll start with the *Single-Agent Training* mode. I'm on a Macbook with Catalina.

First step: follow the mlagents installation guide: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md
Specifically, import the mlagents package from Unity Package Manager, and run `pip3 install mlagents` in a terminal
Note: Unity suggests you run `mlagents-learn --help` to verify your install - this breaks out of the box on Catalina with a cryptic `[libprotobuf ERROR google/protobuf/descriptor_database.cc:394] Invalid file descriptor data passed to EncodedDescriptorDatabase::Add().`. It can be resolved with `pip3 uninstall tensorflow protobuf && pip3 install tensorflow protobuf` (thank you https://github.com/tensorflow/tensorflow/issues/33746).

Second step: I follow the Getting Started guide here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md

There are a few parameters we have to setup in order to start training a bot to play Rusty's Dump Truck Service. 
The Getting Started guide has a good definition of the *Vector Observation Space* and *Vector Action Space*.

It's a bit difficult to make an observation space for Rustybot because unlike the 3D ball demo, where there is a constant set of parameters (the ball's x, y, and z position + velocity), we have a variety of enemies that can come in and out of gameplay and take a variety of actions. That's okay, we'll take a hack at it anyway.

Our Rustybot *Vector Observation Space* must consist of:
The truck's y position
the closest x position of a zone occupant (* 4 since there are four zones)
whether we predict a zones immediately filling completely within the next 2 seconds (as is the case when an alien ship enters and fires a laser) -- again * 4 since there are 4 zones
the x-and-y of a projectile's position (or -1, -1 for no projectile)
This gives us a total `Observation Space Size` of 1 + 4 + 4 + 2 = 11 

Our action space is much simpler! It consists only of:
1. Is the mouse down?
2. The mouse's y input delta

Okay! Let's set this up in our Unity scene.

First we need a RustyBot.cs which is a subclass of an Agent.
We can override the `OnActionReceived` function of Agent to process actions that come from our bot.
For testing, we can also override the `Heuristic` function to allow us to simulate an agent using mouse input.

```
public override void OnActionReceived(float[] vectorAction)
{
    isInputDown = Mathf.Approximately(1f, vectorAction[0]);
    inputYPosition = vectorAction[1];
}
```

```
public override void Heuristic(float[] actionsOut)
{
    actionsOut[0] = Input.GetMouseButton(0) ? 1f : 0f;
    actionsOut[1] = Input.mousePosition.y;
}
```

Next we just need to make the truck take its input from an agent rather than the mouse in-editor.

[Do that here]

Results are... underwhelming, our heuristic function never gets called so our inputs are never set.
Turns out it's because we need a DecisionRequester component attached to our bot. With that attached, our input starts working perfectly as before, with mouse input.

Before committing, I go ahead and add a lose condition to the `OnActionReceived` handler, which rewards RustyBot for every frame it stays alive, and gives it a negative reward on death.

```
public override void OnActionReceived(float[] vectorAction)
{
    isInputDown = Mathf.Approximately(1f, vectorAction[0]);
    inputYPosition = vectorAction[1];

    // make sure we haven't died
    if (truck.GetComponent<Damageable>().Dead)
    {
        SetReward(-1f);
        EndEpisode();
    }
    else
    {
        // survival reward
        SetReward(0.1f);
    }
}
```

With our inputs working now, we need to give RustyBot eyes into our gameworld via observations.


Projectiles and lasers will be a little tricky to hook up so I start with just the phsyical obstacles in each zone.

```
public override void CollectObservations(VectorSensor sensor)
{
    sensor.AddObservation(transform.position.y); // Truck's y position
    foreach(var distance in obstacleCycler.GetObstacleDistancesForCombatZones())
    {
        sensor.AddObservation(distance);    // record the closest enemy for each gameplay zone
        sensor.AddObservation(-1);          // ignore the laser beams since idk how to do that yet
    }
}
```

Okay things look good with heuristic input, let's start training a model.

First, I create a yaml config file for the trainer. The Getting Started guide is a bit light on what this config does, so for now I just copy the `resampling-interval: 5000` out of the 3D ball config and call it a day.

Next I start the trainer using this yaml file:
`mlagents-learn rustybot.yaml --run-id=firstrustyrun`

Okay that didn't work, we get this error:
```
'Trainer config must have either a "default" section, or a section for the brain name ({brain_name}). '
mlagents.trainers.exception.TrainerConfigError: Trainer config must have either a "default" section, or a section for the brain name (TruckBehavior). See config/trainer_config.yaml for an example.
```

Simple enough, I copy the `default` section from [TODO where?]
```
default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
```

[TODO copy final brain file]

Note: my bot wasn't training for a while. That's because the brain was only inputting an inputY position of -1...1 -- whereas my agent code expected 0...Screen.height, which is of course impossible to use, so I ended up normalizing it.

Okay, normalizing the inputs, the truck actually starts moving up and down under the control of an ML agent, nice!
I let it run overnight and this was the result: ![First Attempt](https://www.dropbox.com/s/dsretfs47m8ur7o/rustybot_attempt_1.m4v?raw=1)


