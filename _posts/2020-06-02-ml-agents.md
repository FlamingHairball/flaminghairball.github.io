---
title:  "Training my ML Agent with Unity"
date:   2020-06-17 23:56:10 -0800
categories: dts game
---

# Training my ML Agent with Unity
With *Rusty's Dump Truck Service* headed to the [App Store](https://apps.apple.com/us/app/id1513836293), I wanted to take Unity's ML agents for a spin and train them to pilot Rusty's truck to victory.

The goal is simple - survive as long as possible by avoiding running into an enemy or getting shot down.
The player can move the truck up and down to avoid enemies by dragging a finger. Whenever a finger is on the screen, the truck's gun fires, which destroys the alien baddies.
Alien baddies spawn into 4 discrete zones (top, middle, middle, and bottom).

![Video of gameplay](
https://www.dropbox.com/s/br2qsnc6bo0d5hl/rustybot_heuristic.m4v?raw=1)

This is simple enough that adapting a RustyBot to pilot the truck should be dead simple!

Unity has an intro to the different kinds of ML agents/what they are [here](https://unity3d.com/how-to/unity-machine-learning-agents).

Dump Truck Service is a single-player game, so I'll start with the *Single-Agent Training* mode. I'm on a Macbook with Catalina.

First step: follow the [mlagents installation guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md).
Specifically, import the mlagents package from Unity Package Manager, and run `pip3 install mlagents` in a shell.
Note: Unity suggests you run `mlagents-learn --help` to verify your install - this breaks out of the box on Catalina with a cryptic `[libprotobuf ERROR google/protobuf/descriptor_database.cc:394] Invalid file descriptor data passed to EncodedDescriptorDatabase::Add().`. It can be resolved with `pip3 uninstall tensorflow protobuf && pip3 install tensorflow protobuf` (thank you [github](https://github.com/tensorflow/tensorflow/issues/33746)).

Second step: I follow the Getting Started guide [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md).

There are a few parameters we have to setup in order to start training a bot to play Rusty's Dump Truck Service. 
The Getting Started guide has a good definition of the *Vector Observation Space* and *Vector Action Space*.

It's a bit difficult to make an observation space for Rustybot because unlike the 3D ball demo, where there is a constant set of parameters (the ball's x, y, and z position + velocity), we have a variety of enemies that can come in and out of gameplay and take a variety of actions. That's okay, we'll take a hack at it anyway.

Our Rustybot *Vector Observation Space* must consist of:
The truck's y position
the closest x position of a zone occupant (* 4 since there are four zones)
whether we predict a zones immediately filling completely within the next 2 seconds (as is the case when an alien ship enters and fires a laser) -- again * 4 since there are 4 zones
the x-and-y of a projectile's position (or -1, -1 for no projectile)
This gives us a total `Observation Space Size` of 1 + 4 + 4 + 2 = 11 

Our action space is much simpler! It consists only of:
1. Is the mouse down?
2. The mouse's y input delta

Okay! Let's set this up in our Unity scene.

First we need a RustyBot.cs which is a subclass of an Agent.
We can override the `OnActionReceived` function of Agent to process actions that come from our bot.
For testing, we can also override the `Heuristic` function to allow us to simulate an agent using mouse input.

```
public override void OnActionReceived(float[] vectorAction)
{
    isInputDown = Mathf.Approximately(1f, vectorAction[0]);
    inputYPosition = vectorAction[1];
}
```

```
public override void Heuristic(float[] actionsOut)
{
    actionsOut[0] = Input.GetMouseButton(0) ? 1f : 0f;
    actionsOut[1] = Input.mousePosition.y;
}
```

I hit Play at this point and was pretty confused when my bot didn't do anything. It turns out it's because we need a DecisionRequester component attached to our bot. This was explained in [a useful tutorial that I wish I'd found earlier](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md).
With the DecisionRequester attached, our heuristic mouse-driven input works perfectly (same as before I started this project).

At this point I go ahead and add logic to the `OnActionReceived` handler, which rewards RustyBot for every frame it stays alive, and gives it a negative reward on death.

```
public override void OnActionReceived(float[] vectorAction)
{
    isInputDown = Mathf.Approximately(1f, vectorAction[0]);
    inputYPosition = vectorAction[1];

    // make sure we haven't died
    if (truck.GetComponent<Damageable>().Dead)
    {
        SetReward(-1f);
        EndEpisode();
    }
    else
    {
        // survival reward
        SetReward(0.1f);
    }
}
```

With our inputs working now, we need to give RustyBot eyes into our gameworld via observations.


Projectiles and lasers will be tricky to model, so for now I'll only handle just the phsyical obstacles in each zone.

```
public override void CollectObservations(VectorSensor sensor)
{
    sensor.AddObservation(transform.position.y); // Truck's y position
    foreach(var distance in obstacleCycler.GetObstacleDistancesForCombatZones())   // the distance of an obstacle in each of the 4 zones, infinity if there is no obstacle
    {
        sensor.AddObservation(distance);    // record the closest enemy for each gameplay zone
        sensor.AddObservation(-1);          // ignore the laser beams since idk how to do that yet
    }
}
```

Okay things look good with heuristic input, let's start training a model.

First, I create a yaml config file for the trainer. The Getting Started guide is a bit light on what this config does, so for now I just copy the `resampling-interval: 5000` out of the 3D ball config and call it a day.

Next I start the trainer using this yaml file:
`mlagents-learn rustybot.yaml --run-id=firstrustyrun`

Okay that didn't work, I get this error:
```
'Trainer config must have either a "default" section, or a section for the brain name ({brain_name}). '
mlagents.trainers.exception.TrainerConfigError: Trainer config must have either a "default" section, or a section for the brain name (TruckBehavior). See config/trainer_config.yaml for an example.
```

Simple enough, I copy the `default` section from the [sample here](https://github.com/Unity-Technologies/ml-agents/blob/master/config/ppo/3DBall.yaml).
```
default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
```

I don't really know what this config means, I'll look it up later.

Running `mlagents-learn rustybot.yaml --run-id=firstrustyrun --force` works now (note: I've added `--force` to overwrite the last run which error-ed out)

With the trainer running in my terminal, I press play in Unity and... the truck still doesn't move around. What gives?
After a bit of digging in the editor, I notice that the bot is only experimenting with values that range -1.0-1.0. That makes sense -- but my Rustybot script expects screen-space values (e.g 0.0-768.0) for the mouse position. That's easy enough to fix -- I adapted Rustybot to take normalized mouse position inputs.

When I restart training with normalized inputs, I can see that while the bot runs into aliens quite often, it is starting to move up and down.

I left Rustybot to train overnight on my laptop. 
When you kill the training process, it spits out a .nn file that you can drop into Unity and attach to your ml behavior. This is the result: ![First Attempt](https://www.dropbox.com/s/dsretfs47m8ur7o/rustybot_attempt_1.m4v?raw=1)


Okay it's not skynet but it's a decent start! Next steps are to understand what all those config paramaters mean, and how to use `tensorboard` to understand what's going on during training. And hopefully eventually understand why Rustybot is a pacifist that chooses to avoid killing!

## Parting thoughts
I look forward to seeing ML agents expand, improve, and integrate better with Unity patterns -- it feels bad right now that in spite of Unity having nice level serialization out of the box, setting up an ML training session requires you to essentially "re-implement this" by resetting all your level state manually at the beginning of a training episode. This won't be an issue if you design your levels from the start with an ML training session in mind, but it's a big speedbump for developers trying to get a quick start on an existing project.

Slight inconsistencies/holes in documentation, and broken-out-of-the-box tutorials not withstanding, Unity's done a really great job of making ML accessible to game developers like me with not-a-lot of ML experience. For that I'm super happy.